{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining from PubChem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import os \n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import r2_score\n",
    "# import seaborn as seabornInstance \n",
    "# from sklearn.model_selection import train_test_split \n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn import metrics\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "import time\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio.PDB import *\n",
    "from Bio import ExPASy\n",
    "import os, shutil\n",
    "from biopandas.pdb import PandasPdb\n",
    "from Bio import ExPASy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from Bio import SwissProt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste input in a form of FASTA sequence: MKLFLLLLFLLHISHTFTASRPISEFRALLSLKTSLTGAGDDKNSPLSSWKVSTSFCTWIGVTCDVSRRHVTSLDLSGLNLSGTLSPDVSHLRLLQNLSLAENLISGPIPPEISSLSGLRHLNLSNNVFNGSFPDEISSGLVNLRVLDVYNNNLTGDLPVSVTNLTQLRHLHLGGNYFAGKIPPSYGSWPVIEYLAVSGNELVGKIPPEIGNLTTLRELYIGYYNAFEDGLPPEIGNLSE\n",
      "Query sent to NCBIWWW against swissprot database\n",
      "Top hits: ['O49545', 'Q9M2Z1', 'O65440']\n",
      "Target not found in database\n",
      "Target not found in database\n"
     ]
    }
   ],
   "source": [
    "# protein_query = \"MAMLQTNLGFITSPTFLCPKLKVKLNSYLWFSYRSQVQKLDFSKRVNRSYKRDALLLSIKCSSSTGFDNSNVVVKEKSVSVILLAGGQGKRMKMSMPKQYIPLLGQPIALYSFFTFSRMPEVKEIVVVCDPFFRDIFEEYEESIDVDLRFAIPGKERQDSVYSGLQEIDVNSELVCIHDSARPLVNTEDVEKVLKDGSAVGAAVLGVPAKATIKEVNSDSLVVKTLDRKTLWEMQTPQVIKPELLKKGFELVKSEGLEVTDDVSIVEYLKHPVYVSQGSYTNIKVTTPDDLLLAERILSEDS\"\n",
    "\n",
    "# nucleotide_query = \"ATGGCGATGCTTCAGACGAATCTTGGCTTCATTACTTCTCCGACATTTCTGTGTCCGAAGCTTAAAGTCAAATTGAACTCTTATCTGTGGTTTAGCTATCGTTCTCAAGTTCAAAAACTGGATTTTTCGAAAAGGGTTAATAGAAGCTACAAAAGAGATGCTTTATTATTGTCAATCAAGTGTTCTTCATCGACTGGATTTGATAATAGCAATGTTGTTGTGAAGGAGAAGAGTGTATCTGTGATTCTTTTAGCTGGAGGTCAAGGCAAGAGAATGAAAATGAGTATGCCAAAGCAGTACATACCACTTCTTGGTCAGCCAATTGCTTTGTATAGCTTTTTCACGTTTTCACGTATGCCTGAAGTGAAGGAAATTGTAGTTGTATGTGATCCTTTTTTCAGAGACATTTTTGAAGAATACGAAGAATCAATTGATGTTGATCTTAGATTCGCTATTCCTGGCAAAGAAAGACAAGATTCTGTTTACAGTGGACTTCAGGAAATCGATGTGAACTCTGAGCTTGTTTGTATCCACGACTCTGCCCGACCATTGGTGAATACTGAAGATGTCGAGAAGGTCCTTAAAGATGGTTCCGCGGTTGGAGCAGCTGTACTTGGTGTTCCTGCTAAAGCTACAATCAAAGAGGTCAATTCTGATTCGCTTGTGGTGAAAACTCTCGACAGAAAAACCCTATGGGAAATGCAGACACCACAGGTGATCAAACCAGAGCTATTGAAAAAGGGTTTCGAGCTTGTAAAAAGTGAAGGTCTAGAGGTAACAGATGACGTTTCGATTGTTGAATACCTCAAGCATCCAGTTTATGTCTCTCAAGGATCTTATACAAACATCAAGGTTACAACACCTGATGATTTACTGCTTGCTGAGAGAATCTTGAGCGAGGACTCATGA\"\n",
    "\n",
    "query = Seq(input(\"Paste input in a form of FASTA sequence: \"))\n",
    "\n",
    "# def blast_input():\n",
    "# my_query = Seq(nucleotide_query)\n",
    "# query = SeqIO.read(\"sample.fasta\", format=\"fasta\")\n",
    "try:\n",
    "    print(\"Query sent to NCBIWWW against swissprot database\")\n",
    "    result_handle = NCBIWWW.qblast(\"blastp\", \"swissprot\", query)\n",
    "    blast_result = open(\"my_blast_result.xml\", \"w\")\n",
    "    blast_result.write(result_handle.read())\n",
    "    blast_result.close()\n",
    "    result_handle.close()\n",
    "except ValueError:\n",
    "    my_translated_query = my_query.translate()\n",
    "    print(f\"DNA/RNA sequence detected. Sequence translated to : {my_translated_query}\")\n",
    "    result_handle = NCBIWWW.qblast(\"blastp\", \"swissprot\", my_translated_query)\n",
    "    blast_result = open(\"my_blast_result.xml\", \"w\")\n",
    "    blast_result.write(result_handle.read())\n",
    "    blast_result.close()\n",
    "    result_handle.close()\n",
    "\n",
    "\n",
    "# def read_blast_output():\n",
    "result = open(\"my_blast_result.xml\")\n",
    "blast_records = NCBIXML.parse(result)\n",
    "number_of_alignments = 0\n",
    "hit_ids = []\n",
    "for rec in blast_records:\n",
    "    for alignment in rec.alignments:\n",
    "        hit_ids.append(alignment.accession)\n",
    "        number_of_alignments += 1\n",
    "        if number_of_alignments == 3: # Take three top alignments and stop\n",
    "            break\n",
    "print(f\"Top hits: {hit_ids}\")\n",
    "# return hit_ids\n",
    "if len(hit_ids) == 0:\n",
    "    print(\"No hits identified in the database. Check your FASTA input.\")\n",
    "# def read_target_db(input_list: list):\n",
    "with open('target', 'r') as df:\n",
    "    for line in df:\n",
    "        line = line.split()\n",
    "        try:\n",
    "            for entry in hit_ids:\n",
    "                if entry in line[2]:\n",
    "                    print(f\"Entry {entry} present in database!\")\n",
    "                    check_id = entry\n",
    "                    break\n",
    "        except:\n",
    "            print(\"Target not found in database\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     blast_input()\n",
    "#     read_target_db(read_blast_output())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = \"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=csv&query={%22download%22:%22*%22,%22collection%22:%22bioactivity%22,%22where%22:{%22ands%22:[{%22protacxn%22:%22notnull%22},{%22cid%22:%22notnull%22},{%22repacxn%22:%22P0C6U8%22}]},%22order%22:[%22activity,asc%22],%22start%22:1,%22limit%22:10000000,%22downloadfilename%22:%22{PROTACXN_P0C6U8}_bioactivity_protein%22}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=csv&query={%22download%22:%22*%22,%22collection%22:%22bioactivity%22,%22where%22:{%22ands%22:[{%22protacxn%22:%22notnull%22},{%22cid%22:%22notnull%22},{%22repacxn%22:%22P0C6U8%22}]},%22order%22:[%22activity,asc%22],%22start%22:1,%22limit%22:10000000,%22downloadfilename%22:%22{Q9Y2G1}_bioactivity_protein%22}'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link1 = link1.replace('PROTACXN_P0C6U8', check_id)\n",
    "link1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    try:\n",
    "        os.remove('downloaded1.csv')\n",
    "        #print(\"Deleted old File\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        #print(\"No file \")\n",
    "        break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    try:\n",
    "        data = pd.read_csv(link1)\n",
    "        break\n",
    "    #except IncompleteRead as I:\n",
    "     #   print(\"Server Overloading , Proceeding\")\n",
    "      #  break\n",
    "    except Exception as a:\n",
    "        print(str(a)+\" is the error , Trying {} time\".format(i))\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "else:\n",
    "    print(\"something Wrong , Try running Again [refer error code for more]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"downloaded1.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning up\n",
    "new_data = data[['cid','acvalue']]\n",
    "new_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_val=0\n",
    "cid_value = new_data['cid'].to_list()\n",
    "PIC50_value = (-np.log10(new_data['acvalue']*10**-6)).to_list()\n",
    "\n",
    "PIC50_value = pd.DataFrame(PIC50_value,columns = [\"y\"])\n",
    "y_data = PIC50_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WebAPI PubChem\n",
    "\n",
    "link = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/new_link_me/property/MolecularWeight,HeavyAtomCount,XLOGP,Complexity,HBondAcceptorCount,MonoisotopicMass,RotatableBondCount,TPSA/CSV\"\n",
    "link_fixed = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/new_link_me/property/MolecularWeight,HeavyAtomCount,XLOGP,Complexity,HBondAcceptorCount,MonoisotopicMass,RotatableBondCount,TPSA/CSV\"\n",
    "sub_link = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/fastsimilarity_2d/cid/replaceme/cids/TXT\"\n",
    "sub_link_fixed = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/fastsimilarity_2d/cid/replaceme/cids/TXT\"\n",
    "#the link with the required properties.\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame = pd.DataFrame()\n",
    "x_data = pd.DataFrame()\n",
    "cid_main_counter=0 \n",
    "for x in cid_value:\n",
    "    for iter_x in range(1000):\n",
    "        try:    \n",
    "            link = link_fixed\n",
    "            link = link.replace(\"new_link_me\",str(x))\n",
    "            data1 = pd.read_csv(link)\n",
    "            data1 = pd.DataFrame(data1)\n",
    "            x_data = x_data.append(data1)\n",
    "            data1 = 0\n",
    "        except Exception as e:\n",
    "            print(\"Exception Encountered as {} .! , Trying again !Iteration : {}\".format(str(e),str(iter_x)))\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        print(\"Something Wrong with the Trials ! Restart The Algorithm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_saved = x_data\n",
    "x_data = x_data_saved\n",
    "n_cid=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 sec Wait_time\n",
    "for i in cid_value:\n",
    "    max_tries = 10\n",
    "    \n",
    "    for iter_ in range(max_tries):\n",
    "        try:\n",
    "            #print(\"inside Try\")\n",
    "            #time.sleep(5)\n",
    "\n",
    "\n",
    "            link = link_fixed\n",
    "            link = link.replace(\"new_link_me\",str(i))\n",
    "            f_data = pd.read_csv(link) \n",
    "            \n",
    "            sub_link = sub_link_fixed\n",
    "            sub_link = sub_link.replace(\"replaceme\",str(i))\n",
    "            res = urllib.request.urlopen(sub_link)\n",
    "            data_sub = res.read()\n",
    "            data_sub = str(data_sub)\n",
    "            data_sub = data_sub.replace(\"\\\\n\",\",\") ; data_sub = data_sub.replace('b',\" \")\n",
    "            data_sub = data_sub.replace(\"'\",\"\"); data_sub = data_sub.replace(\" \",\"\")\n",
    "            n_count_loop = 0 \n",
    "            #time.sleep(3)\n",
    "            #print(\"Going for the loop\")\n",
    "            for j in data_sub.split(\",\"):\n",
    "                n_count_loop+=1\n",
    "                if(len(j)>1):\n",
    "                    if(n_count_loop<=30):\n",
    "                        n_cid.append(j)\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    pass\n",
    "             \n",
    "        except:\n",
    "            if(except_val>=15):\n",
    "                break\n",
    "            else:\n",
    "                print(\"Re-Trying\")\n",
    "                except_val+=1\n",
    "                #time.sleep(3)\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    '''else:\n",
    "        print(\"Either The Network is Down(So no point in continuing ) , Or Some Uknown Error Spotted!. Refer Error Code , Continuing with Fetched Data\")\n",
    "        time.sleep(5)\n",
    "        break'''\n",
    "    \n",
    "    \n",
    "    '''final_data_frame = final_data_frame.append(f_data)\n",
    "    cid_main_counter+=1\n",
    "    final_data_frame.to_csv(\"fdf1.csv\")\n",
    "         \n",
    "   \n",
    "\n",
    "    print(\"Main cid no: {}\".format(cid_main_counter))\n",
    "    #time.sleep(5)'''\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_count = 0\n",
    "t_count = 0\n",
    "phase_count=0\n",
    "f_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data mining PubChem through web API\n",
    "\n",
    "for k in n_cid:\n",
    "    k_count+=1\n",
    "    t_count+=1\n",
    "    phase_count+=1\n",
    "    for sub_iter in range(10):\n",
    "            try:\n",
    "                if(t_count>=50):\n",
    "                    #print(\"taking Rest\")\n",
    "                    time.sleep(15)\n",
    "                    t_count=0\n",
    "                    \n",
    "                #print(\"Inside Try\"+str(k_count))\n",
    "            \n",
    "                link = link_fixed\n",
    "                link = link.replace(\"new_link_me\",str(k))\n",
    "                #print(\"replacement done\"+str(k_count))\n",
    "                f_data_df = pd.read_csv(link)\n",
    "                f_data = f_data.append(f_data_df)\n",
    "                print(\"Passed Without Exception\"+str(k_count))\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e) + \"Encountered , Please Wait :+ \"+str(k_count))\n",
    "                time.sleep(20)\n",
    "                continue\n",
    "\n",
    "    final_data_frame = final_data_frame.append(f_data)\n",
    "    cid_main_counter+=1\n",
    "    final_data_frame.to_csv(\"fdf1.csv\")\n",
    "    print(\"Data Fetching Continued\"+ str(k_count))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame.drop_duplicates(inplace=True)\n",
    "print(final_data_frame.shape)\n",
    "print(\"The Final Training DataSet: X_data = {} , Y_data = {}\".format(x_data.shape,y_data.shape))\n",
    "print(\"The Final Data For Preditcion is : {}\".format(final_data_frame.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_data_frame.reset_index(inplace=True)\n",
    "    final_data_frame.drop('index',axis=1,inplace=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame_reset = final_data_frame\n",
    "x_data_reset = x_data\n",
    "y_data_reset = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto QSAR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#RUN TO RESET ONLY : TESTING CAUTION\n",
    "final_data_frame = final_data_frame_reset\n",
    "x_data = x_data_reset\n",
    "y_data = y_data_reset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation for QSAR model building\n",
    "x_data = pd.DataFrame(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.DataFrame(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X :- \"+str(x_data.shape)+\" Y :- \"+str(y_data.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.DataFrame()\n",
    "x_data = x_data.astype(\"float64\")\n",
    "y_data = y_data.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#molecular features for AutoQSAR model building\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimental inhibition value for model building\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = y_data['y'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file  = x_data\n",
    "train_file['y'] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_reg_list=train_file['CID'].to_list()\n",
    "train_file.drop('CID',axis=1,inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_file.isnull().any():\n",
    "    if x == True:\n",
    "        train_file = train_file.fillna(method='ffill')\n",
    "train_file\n",
    "\n",
    "#data set of molecular features and experimental inhibition value for QSAR model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(train_file.columns)\n",
    "x = x[:-1]\n",
    "\n",
    "x_ = train_file[x].values\n",
    "y_ = train_file['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.describe()\n",
    "x_ = x_.astype(\"float64\")\n",
    "y_ = y_.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_, y_, test_size=0.1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AutoQSAR workflow\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "degree=1\n",
    "polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "polyreg_scaled.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 Score\n",
    "y_pred = polyreg_scaled.predict(X_test)\n",
    "main_r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "max_ = main_r2 \n",
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "comb_list = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating combinations , function\n",
    "def sub(arr,r):\n",
    "    global comb_list\n",
    "    for i in r:\n",
    "        comb = list(combinations(arr,i))\n",
    "        comb_list.append(comb)\n",
    "    return comb_list\n",
    "newone = 0\n",
    "newone = sub(x , [2,3,4,5,6])\n",
    "del newone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QSAR model building based on linear multivariate regression for all possible combinations of descriptors and choosing the model with best statistical quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree 1 \n",
    "coef_dict  = 0\n",
    "loop_index = 0\n",
    "coef_dict = [[]]\n",
    "r2_score_new =[]\n",
    "max_r2 = []\n",
    "index_r2 =[]\n",
    "for i in range(0,len(newone)):\n",
    "    \n",
    "    index_r=0\n",
    "    for combi_ in newone[loop_index]:\n",
    "        \n",
    "        #print(combi_)\n",
    "        features = list(combi_)\n",
    "        features_=train_file[features].values\n",
    "        output_=train_file['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_, output_, test_size=0.1, random_state=0)\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        degree=1\n",
    "        polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "        polyreg_scaled.fit(X_train,y_train)\n",
    "        y_pred = polyreg_scaled.predict(X_test)\n",
    "        r2_score_  = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "        r2_score_new.append(r2_score_)\n",
    "    loop_index+=1\n",
    "        \n",
    "    \n",
    "    max_r2.append(max(r2_score_new))\n",
    "    index_r = r2_score_new.index(max(r2_score_new))\n",
    "    index_r2.append(index_r)\n",
    "    r2_score_new =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best value so far using the regressor\n",
    "sec_index = max_r2.index(max(max_r2))\n",
    "fir_index = index_r2[sec_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if main_r2 > max(max_r2):\n",
    "    r2_features = x\n",
    "else:\n",
    "    \n",
    "    features  = newone[sec_index][fir_index]\n",
    "    maxi_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 - degree1 = \" + str(maxi_r2)+str(\"\\n\")+str(\"Features = \")+str(features))\n",
    "reg_max_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have the best degree 1 model\n",
    "model_dict = {}\n",
    "model_dict[1]=[maxi_r2,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_file.isnull().any():\n",
    "    if x == True:\n",
    "        train_file = train_file.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QSAR model building based on non-linear multivariate regression(degree 2) for all possible combinations of descriptors and choosing the model with best statistical quality \n",
    "coef_dict  = 0\n",
    "loop_index = 0\n",
    "coef_dict = [[]]\n",
    "r2_score_new =[]\n",
    "max_r2 = []\n",
    "index_r2 =[]\n",
    "for i in range(0,len(newone)):\n",
    "    \n",
    "    index_r=0\n",
    "    for combi_ in newone[loop_index]:\n",
    "        \n",
    "        #print(combi_)\n",
    "        features = list(combi_)\n",
    "        features_=train_file[features].values\n",
    "        output_=train_file['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_, output_, test_size=0.1, random_state=0)\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        degree=2\n",
    "        polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "        polyreg_scaled.fit(X_train,y_train)\n",
    "        y_pred = polyreg_scaled.predict(X_test)\n",
    "        r2_score_  = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "        r2_score_new.append(r2_score_)\n",
    "    loop_index+=1\n",
    "        \n",
    "    \n",
    "    max_r2.append(max(r2_score_new))\n",
    "    index_r = r2_score_new.index(max(r2_score_new))\n",
    "    index_r2.append(index_r)\n",
    "    r2_score_new =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best value so far using the regressor\n",
    "sec_index = max_r2.index(max(max_r2))\n",
    "fir_index = index_r2[sec_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if main_r2 > max(max_r2):\n",
    "    r2_features = x\n",
    "else:\n",
    "    \n",
    "    features  = newone[sec_index][fir_index]\n",
    "    maxi_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 - degree2 = \" + str(maxi_r2)+str(\"\\n\")+str(\"Features = \")+str(features))\n",
    "reg_max_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[2]=[maxi_r2,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_file.isnull().any():\n",
    "    if x == True:\n",
    "        train_file = train_file.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QSAR model building based on non-linear multivariate regression(degree 3) for all possible combinations of descriptors and choosing the model with best statistical quality \n",
    "\n",
    "Data mining from PubChem\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import urllib\n",
    "\n",
    "import os \n",
    "\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import seaborn as seabornInstance \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "import time\n",
    "\n",
    "link1 = \"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=csv&query={%22download%22:%22*%22,%22collection%22:%22bioactivity%22,%22where%22:{%22ands%22:[{%22protacxn%22:%22notnull%22},{%22cid%22:%22notnull%22},{%22repacxn%22:%22P0C6U8%22}]},%22order%22:[%22activity,asc%22],%22start%22:1,%22limit%22:10000000,%22downloadfilename%22:%22PROTACXN_P0C6U8_bioactivity_protein%22}\"\n",
    "\n",
    "for i in range(0,2):\n",
    "\n",
    "    try:\n",
    "\n",
    "        os.remove('downloaded1.csv')\n",
    "\n",
    "        #print(\"Deleted old File\")\n",
    "\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        #print(\"No file \")\n",
    "\n",
    "        break\n",
    "\n",
    "    else:\n",
    "\n",
    "        break\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    try:\n",
    "\n",
    "        data = pd.read_csv(link1)\n",
    "\n",
    "        break\n",
    "\n",
    "    #except IncompleteRead as I:\n",
    "\n",
    "     #   print(\"Server Overloading , Proceeding\")\n",
    "\n",
    "      #  break\n",
    "\n",
    "    except Exception as a:\n",
    "\n",
    "        print(str(a)+\" is the error , Trying {} time\".format(i))\n",
    "\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "\n",
    "        break\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"something Wrong , Try running Again [refer error code for more]\")\n",
    "\n",
    "#data = pd.read_csv(\"downloaded1.csv\",error_bad_lines=False)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data\n",
    "\n",
    "\tbaid \tactivity \taid \tsid \tmid \tcid \tgeneid \ttaxid \tpmid \taidtype \t... \tacname \tacvalue \taidsrcname \taidname \tcmpdname \ttargetname \ttargeturl \tdois \tecs \trepacxn\n",
    "0 \t98754968 \tActive \t241511 \t103458247 \t0 \t11667869 \tNaN \t227859 \t15896959 \tConfirmatory \t... \tIC50 \t0.98 \tChEMBL \tIn vitro inhibitory concentration SARS coronav... \t1-Benzo[b]thiophen-2-ylmethyl-7-bromo-1H-indol... \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmcl.2005.04.027 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "1 \t98758767 \tActive \t241749 \t103458062 \t0 \t44398002 \tNaN \t227859 \t15896959 \tConfirmatory \t... \tIC50 \t9.40 \tChEMBL \tIn vitro inhibitory concentration against SARS... \t1-(2-Chloro-4-fluoro-benzyl)-5-iodo-1H-indole-... \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmcl.2005.04.027 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "2 \t98758768 \tActive \t241749 \t103458580 \t0 \t44398207 \tNaN \t227859 \t15896959 \tConfirmatory \t... \tIC50 \t4.82 \tChEMBL \tIn vitro inhibitory concentration against SARS... \t1-Benzo[b]thiophen-2-ylmethyl-5-fluoro-1H-indo... \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmcl.2005.04.027 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "3 \t98758769 \tActive \t241749 \t103458848 \t0 \t44398343 \tNaN \t227859 \t15896959 \tConfirmatory \t... \tIC50 \t2.00 \tChEMBL \tIn vitro inhibitory concentration against SARS... \t1-Benzo[b]thiophen-2-ylmethyl-7-nitro-1H-indol... \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmcl.2005.04.027 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "4 \t98758770 \tActive \t241749 \t103459046 \t0 \t44398436 \tNaN \t227859 \t15896959 \tConfirmatory \t... \tIC50 \t17.50 \tChEMBL \tIn vitro inhibitory concentration against SARS... \t5-Iodo-1-[[5-(piperidine-1-carbonyl)thiophen-2... \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmcl.2005.04.027 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t...\n",
    "336 \t424668663 \tUnspecified \t688306 \t163312872 \t0 \t14610613 \tNaN \t227859 \t22884354 \tLiterature-derived \t... \tNaN \tNaN \tChEMBL \tTime dependent inhibition of SARS-CoV PLpro ex... \tMethyl tanshinonate \tReplicase polyprotein 1a (SARS coronavirus) \t/protein/P0C6U8 \t10.1016/j.bmc.2012.07.038 \t3.4.19.12,3.4.22.-,3.4.22.69 \tP0C6U8\n",
    "337 \t424668664 \tUnspecified \t688306 \t103456666 \t0 \t164676 \tNaN \t227859 \t22884354 \tLiterature-derived \t... \tNaN \tNaN \tChEMBL \tTime dependent inhibition of SARS-CoV PLpro ex... \t\n",
    "coef_dict  = 0\n",
    "loop_index = 0\n",
    "coef_dict = [[]]\n",
    "r2_score_new =[]\n",
    "max_r2 = []\n",
    "index_r2 =[]\n",
    "for i in range(0,len(newone)):\n",
    "    \n",
    "    index_r=0\n",
    "    for combi_ in newone[loop_index]:\n",
    "        \n",
    "        #print(combi_)\n",
    "        features = list(combi_)\n",
    "        features_=train_file[features].values\n",
    "        output_=train_file['y'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_, output_, test_size=0.1, random_state=0)\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        degree=3\n",
    "        polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "        polyreg_scaled.fit(X_train,y_train)\n",
    "        y_pred = polyreg_scaled.predict(X_test)\n",
    "        r2_score_  = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "        r2_score_new.append(r2_score_)\n",
    "    loop_index+=1\n",
    "        \n",
    "    \n",
    "    max_r2.append(max(r2_score_new))\n",
    "    index_r = r2_score_new.index(max(r2_score_new))\n",
    "    index_r2.append(index_r)\n",
    "    r2_score_new =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best value so far using the regressor\n",
    "sec_index = max_r2.index(max(max_r2))\n",
    "fir_index = index_r2[sec_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if main_r2 > max(max_r2):\n",
    "    r2_features = x\n",
    "else:\n",
    "    \n",
    "    features  = newone[sec_index][fir_index]\n",
    "    maxi_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 - degree3 = \" + str(maxi_r2)+str(\"\\n\")+str(\"Features = \")+str(features))\n",
    "reg_max_r2 = max(max_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[3]=[maxi_r2,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_file.isnull().any():\n",
    "    if x == True:\n",
    "        train_file = train_file.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction with the validated model\n",
    "model_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(model_dict[1][1])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained =train_file[features].values\n",
    "y_trained = train_file['y'].values\n",
    "x_trained.shape,y_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_trained, y_trained, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly regression - degree 1\n",
    "scaler = preprocessing.StandardScaler()\n",
    "degree=1\n",
    "polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "polyreg_scaled.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 Score\n",
    "y_pred = polyreg_scaled.predict(X_test)\n",
    "r2_score(y_test, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frameF1 = final_data_frame[final_data_frame['MolecularWeight'] <= 500]\n",
    "final_data_frameF2 = final_data_frameF1[final_data_frameF1['XLogP'] <=5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_cid = final_data_frameF2[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.append('CID')\n",
    "new_features = features.append(\"CID\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame1 = final_data_frameF2[features]\n",
    "for x in final_data_frame1.isnull().any():\n",
    "    if x == True:\n",
    "        final_data_frame1 = final_data_frame1.fillna(method='ffill')\n",
    "final_data_frame1.drop(columns='CID',inplace=True)\n",
    "final_data_frame1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_1 = polyreg_scaled.predict(final_data_frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_1 = list(final_pred_1)\n",
    "final_data_frame1['CID'] = pred_data_cid\n",
    "final_data_frame1['y_'] = final_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_final_data_frame1 = final_data_frame1\n",
    "sorted_final_df = final_data_frame1.sort_values('y_',ascending =0)\n",
    "final_larg= sorted_final_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_larg = final_larg[['CID','y_']]\n",
    "top_cid = final_larg[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Top 50 Drug Leads Which are identified with PubChem cid's are (USING Linear REGRESSION): \")\n",
    "itter_count = 0\n",
    "for itter in top_cid:\n",
    "    itter_count+=1\n",
    "    \n",
    "    print(str(itter_count)+\" : \"+str(itter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r2 = \"+str(r2_score(y_test, y_pred, multioutput='uniform_average')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df['Degree 1'] = top_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cid_1 = pd.DataFrame()\n",
    "final_cid_1['CID'] = top_cid\n",
    "final_cid_1.to_csv(\"final_cid_degree_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(model_dict[2][1])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained =train_file[features].values\n",
    "y_trained = train_file['y'].values\n",
    "x_trained.shape,y_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_trained, y_trained, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly regression - degree 2\n",
    "scaler = preprocessing.StandardScaler()\n",
    "degree=2\n",
    "polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "polyreg_scaled.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 Score\n",
    "y_pred = polyreg_scaled.predict(X_test)\n",
    "r2_score(y_test, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frameF1 = final_data_frame[final_data_frame['MolecularWeight'] <= 500]\n",
    "final_data_frameF2 = final_data_frameF1[final_data_frameF1['XLogP'] <=5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_cid = final_data_frameF2[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.append('CID')\n",
    "new_features = features.append(\"CID\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame1 = final_data_frameF2[features]\n",
    "for x in final_data_frame1.isnull().any():\n",
    "    if x == True:\n",
    "        final_data_frame1 = final_data_frame1.fillna(method='ffill')\n",
    "final_data_frame1.drop(columns='CID',inplace=True)\n",
    "final_data_frame1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_1 = polyreg_scaled.predict(final_data_frame1)\n",
    "final_pred_1 = list(final_pred_1)\n",
    "final_data_frame1['CID'] = pred_data_cid\n",
    "final_data_frame1['y_'] = final_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_final_data_frame1 = final_data_frame1\n",
    "sorted_final_df = final_data_frame1.sort_values('y_',ascending =0)\n",
    "final_larg= sorted_final_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_larg = final_larg[['CID','y_']]\n",
    "top_cid = final_larg[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Top 50 Drug Leads Which are identified with PubChem cid's are (USING DEGREE 2 REGRESSION): \")\n",
    "itter_count = 0\n",
    "for itter in top_cid:\n",
    "    itter_count+=1\n",
    "    \n",
    "    print(str(itter_count)+\" : \"+str(itter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df['Degree 2'] = top_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r2 = \"+str(r2_score(y_test, y_pred, multioutput='uniform_average')))\n",
    "final_cid_2 = pd.DataFrame()\n",
    "final_cid_2['CID'] = top_cid\n",
    "final_cid_2.to_csv(\"final_cid_degree_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(model_dict[3][1])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained =train_file[features].values\n",
    "y_trained = train_file['y'].values\n",
    "x_trained.shape,y_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_trained, y_trained, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly regression - degree 3\n",
    "scaler = preprocessing.StandardScaler()\n",
    "degree=3\n",
    "polyreg_scaled=make_pipeline(PolynomialFeatures(degree),scaler,LinearRegression())\n",
    "polyreg_scaled.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 Score\n",
    "y_pred = polyreg_scaled.predict(X_test)\n",
    "r2_score(y_test, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frameF1 = final_data_frame[final_data_frame['MolecularWeight'] <= 500]\n",
    "final_data_frameF2 = final_data_frameF1[final_data_frameF1['XLogP'] <=5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_cid = final_data_frameF2[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.append('CID')\n",
    "new_features = features.append(\"CID\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame1 = final_data_frameF2[features]\n",
    "for x in final_data_frame1.isnull().any():\n",
    "    if x == True:\n",
    "        final_data_frame1 = final_data_frame1.fillna(method='ffill')\n",
    "final_data_frame1.drop(columns='CID',inplace=True)\n",
    "final_data_frame1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_1 = polyreg_scaled.predict(final_data_frame1)\n",
    "final_pred_1 = list(final_pred_1)\n",
    "final_data_frame1['CID'] = pred_data_cid\n",
    "final_data_frame1['y_'] = final_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_final_data_frame1 = final_data_frame1\n",
    "sorted_final_df = final_data_frame1.sort_values('y_',ascending =0)\n",
    "final_larg= sorted_final_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_larg = final_larg[['CID','y_']]\n",
    "top_cid = final_larg[\"CID\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Top 50 Drug Leads Which are identified with PubChem cid's are (USING DEGREE 3 REGRESSION): \")\n",
    "itter_count = 0\n",
    "for itter in top_cid:\n",
    "    itter_count+=1\n",
    "    \n",
    "    print(str(itter_count)+\" : \"+str(itter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df['Degree 3'] = top_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r2 = \"+str(r2_score(y_test, y_pred, multioutput='uniform_average')))\n",
    "final_cid_2 = pd.DataFrame()\n",
    "final_cid_2['CID'] = top_cid\n",
    "final_cid_2.to_csv(\"final_cid_degree_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " THE END - COMPOUND FETCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cid_mix_df.to_csv(\"TOP_CID_123.csv\")\n",
    "\n",
    "\n",
    "\n",
    "top_cid = top_cid_mix_df['Degree 3']\n",
    "\n",
    "top_cid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated In Silico modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResSelect(Select):\n",
    "    def accept_residue(self, residue):\n",
    "        if is_aa(residue):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class ProteinPreparer:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, protein: str, dir: str):\n",
    "        self.protein = protein\n",
    "        self.tmpdir = dir\n",
    "        self.filename = self.tmpdir + '/pdb{}.ent'.format(self.protein)\n",
    "\n",
    "    def prepare_protein(self):\n",
    "        parser = PDBParser()\n",
    "        io = PDBIO()\n",
    "        PDBList().retrieve_pdb_file(pdb_code=self.protein, pdir=self.tmpdir,\n",
    "                                    file_format='pdb')  # saves pdb in a form of ent file\n",
    "        ipdb = parser.get_structure('ipdb', self.filename)  # Input pdb as a self.filename\n",
    "        io.set_structure(ipdb)  # Setting structure for input pdb\n",
    "        io.save(self.tmpdir + '/'+self.protein + '.pdb', ResSelect(),\n",
    "                preserve_atom_numbering=True)  # saves the cleaned pdb\n",
    "        os.system(prepare_protein_path + self.tmpdir + '/{}.pdb'.format(\n",
    "            self.protein))  # prepares the structure by adding polar hydrogens and adding gesteiger charges\n",
    "        shutil.move(self.protein + '.pdbqt', self.tmpdir)\n",
    "\n",
    "prepare_ligand_path = '~/MGLTools-1.5.6/bin/pythonsh ~/MGLTools-1.5.6/MGLToolsPckgs/AutoDockTools/Utilities24/prepare_ligand4.py -A bonds_hydrogens -U nphs_lps -l'\n",
    "\n",
    "class LigandPreparer:\n",
    "\n",
    "    def __init__(self, ligand_file: str, dir: str):\n",
    "        self.ligand = ligand_file\n",
    "        self.tmpdir = dir\n",
    "\n",
    "    def prepare_ligand(self):\n",
    "        url = 'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{}/SDF'.format(self.ligand)\n",
    "        urllib.request.urlretrieve(url, self.tmpdir + '/{}'.format(self.ligand + '.sdf')) # downloads the file\n",
    "        os.system('obabel {} -O {} --gen3d'.format(\n",
    "            self.tmpdir + '/' + self.ligand + '.sdf', self.tmpdir + '/' + self.ligand + 'prep.pdb')\n",
    "        ) # generates 3d coords\n",
    "        os.system(\n",
    "            'obminimize -ff GAFF {} > {}'.format(\n",
    "                self.tmpdir + '/' + self.ligand + 'prep.pdb', self.tmpdir + '/' + self.ligand + '.pdb'\n",
    "                                                 )\n",
    "        ) # minimizes using GAFF\n",
    "        os.system(prepare_ligand_path + '{}'.format(self.tmpdir + '/' + self.ligand + '.pdb'\n",
    "                                                    )) # adds charges, sets rotatable bonds\n",
    "        shutil.move(self.ligand + '.pdbqt', self.tmpdir)\n",
    "\n",
    "class VinaDocker:\n",
    "\n",
    "    def __init__(self, ligentry: str, protentry: str, protein_pdbqt: str, ligand_pdbqt: str, dir: str):\n",
    "        self.protein = protein_pdbqt + '.pdbqt'\n",
    "        self.protpdb = protentry + '.pdb'\n",
    "        self.protname = os.path.basename(self.protein)\n",
    "        self.ligand = ligand_pdbqt + '.pdbqt'\n",
    "        self.ligname = os.path.basename(self.ligand)\n",
    "        self.tmpdir = dir\n",
    "        self.docklog = './results/' + protentry + '_' + ligentry + '_docking.log' # !!\n",
    "        self.dockfile =  './results/' + protentry + '_' + ligentry + '.out' # !!\n",
    "        self.complex_name = protentry + '_' + ligentry + '_cplx.pdb'\n",
    "\n",
    "    def dock_merge_plip(self):\n",
    "        df = PandasPdb().read_pdb(self.tmpdir + '/' + self.protpdb)  # opens protein to calculate grid\n",
    "        minx = df.df['ATOM']['x_coord'].min()\n",
    "        maxx = df.df['ATOM']['x_coord'].max()\n",
    "        cent_x = round((maxx + minx) / 2, 2)\n",
    "        size_x = round(abs(maxx - minx) + 3, 2)\n",
    "        miny = df.df['ATOM']['y_coord'].min()\n",
    "        maxy = df.df['ATOM']['y_coord'].max()\n",
    "        cent_y = round((maxy + miny) / 2, 2)\n",
    "        size_y = round(abs(maxy - miny) + 3, 2)\n",
    "        minz = df.df['ATOM']['z_coord'].min()\n",
    "        maxz = df.df['ATOM']['z_coord'].max()\n",
    "        cent_z = round((maxz + minz) / 2, 2)\n",
    "        size_z = round(abs(maxz - minz) + 3, 2)\n",
    "        assert (type(cent_x) != None), \"Protein structure is damaged\"\n",
    "        assert (type(cent_y) != None), \"Protein structure is damaged\"\n",
    "        assert (type(cent_z) != None), \"Protein structure is damaged\"\n",
    "\n",
    "\n",
    "        print(\"Center point of docking grid for {} is as follows: \"\n",
    "              \"x: {}, y: {}, z: {}\".format(self.protein, size_x, size_y, size_z))\n",
    "        print(\"Sizes of docking grid are as follows:\"\n",
    "              \"x: {}, y: {}, z: {}\".format(cent_x, cent_y, cent_z))\n",
    "        print(\"Receptor: \", self.protein, \"\\n\", \"Ligand: \", self.ligand, \"\\n\", \"Output file: \", self.dockfile)\n",
    "        os.system(\n",
    "            'vina --receptor {} --ligand \"{}\" --center_x {} --center_y {} --center_z {} --size_x {} --size_y {} --size_z {} --log \"{}\" --out \"{}\"'.format(\n",
    "                self.protein,\n",
    "                self.ligand,\n",
    "                cent_x,\n",
    "                cent_y,\n",
    "                cent_z,\n",
    "                size_x,\n",
    "                size_y,\n",
    "                size_z,\n",
    "                self.docklog,\n",
    "                self.dockfile\n",
    "            ))\n",
    "        \"\"\"Postprocessing of docking files\"\"\"\n",
    "        # time.sleep(60)\n",
    "        df.df['ATOM']['segment_id'].replace(r'.{1,}', '', regex=True, inplace=True) # Clean pdbqt inheritance\n",
    "        df.df['ATOM']['blank_4'].replace(r'.{1,}', '', regex=True, inplace=True)\n",
    "        docking_output_df = PandasPdb().read_pdb(self.dockfile)\n",
    "        docking_output_df.df['HETATM'].drop_duplicates(subset='atom_number', keep='first', inplace=True)  # extract first model\n",
    "        docking_output_df.df['HETATM']['segment_id'].replace(r'.{1,}', '', regex=True, inplace=True)\n",
    "        docking_output_df.df['HETATM']['blank_4'].replace(r'.{1,}', '', regex=True, inplace=True)  # clean pdbqt inheritance\n",
    "        df.df['ATOM'] = df.df['ATOM'].append(docking_output_df.df['HETATM'], ignore_index=True) # merges the files\n",
    "        df.to_pdb(path = self.complex_name,\n",
    "                  records=['ATOM','HETATM'],\n",
    "                  gz = False,\n",
    "                  append_newline=True)\n",
    "        shutil.move(self.complex_name, './results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of found pdbs:  ['1O5S', '1P76', '1P9T', '1PA5', '1PUK', '1Q1X', '1Q2W', '1QZ8', '1SXF', '1UJ1', '1UK2', '1UK3', '1UK4', '1UW7', '1WOF', '1YSY', '1Z1I', '1Z1J', '2A5A', '2A5I', '2A5K', '2ACF', '2AHM', '2AJ5', '2ALV', '2AMD', '2AMQ', '2BX3', '2BX4', '2C3S', '2D2D', '2DUC', '2FAV', '2FE8', '2FYG', '2G1F', '2G9T', '2GA6', '2GDT', '2GRI', '2GT7', '2GT8', '2GTB', '2GX4', '2GZ7', '2GZ8', '2GZ9', '2H2Z', '2H85', '2HOB', '2HSX', '2IDY', '2JZD', '2JZE', '2JZF', '2K87', '2OP9', '2OZK', '2PWX', '2Q6G', '2QC2', '2QCY', '2QIQ', '2RHB', '2RNK', '2V6N', '2VJ1', '2XYQ', '2XYR', '2XYV', '2Z3C', '2Z3D', '2Z3E', '2Z94', '2Z9G', '2Z9J', '2Z9K', '2Z9L', '3D62', '3E9S', '3EBN', '3R24', '4TWW', '4TWY', '4WY3', '4ZUH', '5B6O', '5C5N', '5C5O', '5C8S', '5C8T', '5C8U', '5E6J', '5F22', '5N19', '5N5O', '5NFY', '6JYT', '6LNQ', '6NUR', '6NUS']\n",
      "Downloading PDB structure '1o5s'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1p76'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1p9t'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1pa5'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1puk'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1q1x'...\n",
      "Desired structure doesn't exists\n",
      "Downloading PDB structure '1q2w'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5139.\n",
      "  warnings.warn(\n",
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 5381.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDB structure '1qz8'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 2283.\n",
      "  warnings.warn(\n",
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 2298.\n",
      "  warnings.warn(\n",
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 2303.\n",
      "  warnings.warn(\n",
      "/home/rafcie/anaconda3/envs/blast-x-interpreter/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 2320.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point of docking grid for /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/1qz8.pdbqt is as follows: x: 63.74, y: 51.52, z: 55.76\n",
      "Sizes of docking grid are as follows:x: -3.15, y: 64.82, z: 81.9\n",
      "Receptor:  /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/1qz8.pdbqt \n",
      " Ligand:  /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/6500.pdbqt \n",
      " Output file:  ./results/1qz8_6500.out\n",
      "Destination path './results/1qz8_6500_cplx.pdb' already exists\n",
      "Center point of docking grid for /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/1qz8.pdbqt is as follows: x: 63.74, y: 51.52, z: 55.76\n",
      "Sizes of docking grid are as follows:x: -3.15, y: 64.82, z: 81.9\n",
      "Receptor:  /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/1qz8.pdbqt \n",
      " Ligand:  /home/rafcie/PycharmProjects/target2drug/tmphrlkpp_f/6501.pdbqt \n",
      " Output file:  ./results/1qz8_6501.out\n",
      "Destination path './results/1qz8_6501_cplx.pdb' already exists\n",
      "****Docking procedure finished****\n"
     ]
    }
   ],
   "source": [
    "#Dummy CID:\n",
    "# top_cid=[6500, 6501]\n",
    "\n",
    "top_cid=[str(x) for x in top_cid]\n",
    "# check_id='POC6X7'\n",
    "# Part 1 - look for PDB entry\n",
    "\n",
    "list_of_pdbs = []\n",
    "handle = ExPASy.get_sprot_raw(check_id)\n",
    "record = SwissProt.read(handle)\n",
    "for i in record.cross_references:\n",
    "    if i[0] == 'PDB':\n",
    "        list_of_pdbs.append(i[1].upper())\n",
    "        \n",
    "print(\"List of found pdbs: \", list_of_pdbs)\n",
    "#Part 2 - download and prepare PDB\n",
    "prepare_protein_path = '~/MGLTools-1.5.6/bin/pythonsh ~/MGLTools-1.5.6/MGLToolsPckgs/AutoDockTools/Utilities24/prepare_receptor4.py -A bonds_hydrogens -U nphs_lps_waters_nonstdres_deleteAltB -r'\n",
    "with tempfile.TemporaryDirectory(dir=os.getcwd()) as tmpdir:\n",
    "    for entry in list_of_pdbs:\n",
    "        try:\n",
    "            ProteinPreparer(entry.lower(), tmpdir).prepare_protein()\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "#     Part 3 - prepare ligands\n",
    "    for CID_entry in top_cid:\n",
    "        try:\n",
    "            LigandPreparer(CID_entry,tmpdir).prepare_ligand()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#         Part 4 - dock the ligand\n",
    "        try:\n",
    "            for entry in os.listdir(tmpdir):\n",
    "                if (entry[0:4].upper() in list_of_pdbs) and (entry.endswith('.pdbqt')):\n",
    "                    VinaDocker(protentry=entry[0:4].lower(),\n",
    "                                        ligentry=CID_entry,\n",
    "                                        protein_pdbqt=tmpdir + '/' + entry[0:4].lower(),\n",
    "                                        ligand_pdbqt=tmpdir + '/' + CID_entry,\n",
    "                                        dir=tmpdir\n",
    "                                        ).dock_merge_plip()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"****Docking procedure finished****\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n",
      "Processing file ./results/5yhu_6501_cplx.pdb\n",
      "https://projects.biotec.tu-dresden.de/plip-web/plip/download/d4521929-dbef-4d53-b1b6-14e7c9269c3a?filePath=outputs%2F5YHU_6501_CPLX_PROTEIN_UNL_Z_1.png\n",
      "Image saved as ./results/5yhu_6501_cplx.pdb.png\n",
      "https://projects.biotec.tu-dresden.de/plip-web/plip/download/d4521929-dbef-4d53-b1b6-14e7c9269c3a?filePath=outputs%2F5YHU_6501_CPLX_PROTEIN_UNL_Z_1.pse\n",
      "Pymol sessions saved as ./results/5yhu_6501_cplx.pdb.pse\n",
      "Processing file ./results/5yhu_6500_cplx.pdb\n",
      "https://projects.biotec.tu-dresden.de/plip-web/plip/download/2752bfb5-f155-4ba5-85ca-f23ba860346c?filePath=outputs%2F5YHU_6500_CPLX_PROTEIN_UNL_Z_1.png\n",
      "Image saved as ./results/5yhu_6500_cplx.pdb.png\n",
      "https://projects.biotec.tu-dresden.de/plip-web/plip/download/2752bfb5-f155-4ba5-85ca-f23ba860346c?filePath=outputs%2F5YHU_6500_CPLX_PROTEIN_UNL_Z_1.pse\n",
      "Pymol sessions saved as ./results/5yhu_6500_cplx.pdb.pse\n"
     ]
    }
   ],
   "source": [
    "#Part 5 - PLIP\n",
    "#protein-ligand interaction profiling through PLIP \n",
    "options=Options()\n",
    "options.headless = True\n",
    "plip = webdriver.Firefox(options=options)\n",
    "plip.get(\"https://projects.biotec.tu-dresden.de/plip-web/plip\")\n",
    "print(\"Connection successful\")\n",
    "\n",
    "for cplx in os.listdir('./results/'):\n",
    "    if cplx.endswith('cplx.pdb'):\n",
    "        cplx=f'./results/{cplx}'\n",
    "        print(\"Processing file {}\".format(cplx))\n",
    "    \n",
    "        select_pdb_input = plip.find_element_by_xpath(\"//*[@id='select-pdb-by-file']\").click() \n",
    "\n",
    "        browse = plip.find_element_by_xpath(\n",
    "            '/html/body/div[1]/div[2]/div/form/div[1]/div[1]/div[3]/input'\n",
    "        ).send_keys(                                                       \n",
    "            os.getcwd()+'/{}'.format(cplx)\n",
    "        )\n",
    "\n",
    "        send_file = plip.find_element_by_xpath(\"//*[@id='submit']\").click() \n",
    "        time.sleep(10) \n",
    "        try:\n",
    "            try:\n",
    "                open_interactions_1 = plip.find_element_by_xpath('/html/body/div/div[2]/div/div[1]/h2[2]').click()\n",
    "                open_interactions_2 = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div[2]/h3').click()\n",
    "                open_interactions_3 = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div[2]/div/h4').click()\n",
    "                pngs = plip.find_elements_by_xpath(\"//a[contains(@href,'.png')]\")\n",
    "                pymolsessions = plip.find_elements_by_xpath(\"//a[contains(@href,'.pse')]\")\n",
    "                \n",
    "            except:\n",
    "                open_interactions_1 = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/h2[1]').click()\n",
    "                open_interactions_2 = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div[1]/h3').click()\n",
    "                open_interactions_3 = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div[1]/div/h4').click()\n",
    "                pngs = plip.find_elements_by_xpath(\"//a[contains(@href,'.png')]\")\n",
    "                pymolsessions = plip.find_elements_by_xpath(\"//a[contains(@href,'.pse')]\")\n",
    "\n",
    "            for image in pngs:\n",
    "                print(image.get_attribute(\"href\"))\n",
    "                output_image = requests.get(image.get_attribute(\"href\"))\n",
    "                open(\n",
    "                    os.getcwd()+'/{}'.format(cplx+'.png'), 'wb'\n",
    "                ).write(output_image.content)\n",
    "                print(\"Image saved as {}\".format(cplx+'.png'))\n",
    "\n",
    "            for pysession in pymolsessions:\n",
    "                print(pysession.get_attribute(\"href\"))\n",
    "                pse = requests.get(pysession.get_attribute(\"href\"))\n",
    "                open(\n",
    "                    os.getcwd()+'/{}'.format(cplx+'.pse'), 'wb'\n",
    "                ).write(pse.content)\n",
    "                print(\"Pymol sessions saved as {}\".format(cplx+'.pse'))\n",
    "                  \n",
    "            restart_plip = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/p[3]/a').click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print(\"No interactions found for {} or damaged structure\".format(cplx))\n",
    "            try:\n",
    "                restart_plip = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div/p[3]/a').click()\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                restart_plip = plip.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/p/a').click() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT ENDS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
